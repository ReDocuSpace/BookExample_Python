{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: newspaper3k in c:\\users\\gj\\anaconda3\\lib\\site-packages (0.2.8)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (4.12.2)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (1.1.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (3.8.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (2.31.0)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (6.0.10)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (3.2.0)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (0.35.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\gj\\appdata\\roaming\\python\\python311\\site-packages (from newspaper3k) (2.8.2)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from newspaper3k) (0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.4)\n",
      "Requirement already satisfied: six in c:\\users\\gj\\appdata\\roaming\\python\\python311\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\gj\\anaconda3\\lib\\site-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
      "Requirement already satisfied: click in c:\\users\\gj\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gj\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gj\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2023.7.22)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (1.5.1)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\gj\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\gj\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.2.1->newspaper3k) (0.4.6)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title :  '올해는 더 뜨겁다' 스마일게이트, CFS2023 그랜드 파이널 中서 열린다\n",
      "Press :  메트로신문\n",
      "Date :  1일 전\n",
      "Summary :  스마일게이트, CFS 2023 그랜드 파이널 중국 청두서 본격 개막/스마일게이트 스마일게이트 엔터테인먼트는 중국 청두에서 전세계 최고 권위의 '크로스파이어' e스포츠 대회인 'CFS 2023 그랜드 파이널'의 개막식과 개막전을 지난 29일 개최했다고 30일 밝혔다. 해당 대회는 12일 간 진행된다. CFS 2023...\n",
      "Href :  http://www.metroseoul.co.kr/article/20231130500011\n",
      "\n",
      "Title :  G마켓 빅스마일데이, 1020세대 구매 36% 뛰었다\n",
      "Press :  뉴스1\n",
      "Date :  3일 전\n",
      "Summary :  G마켓은 연중 최대 온라인 쇼핑 축제 빅스마일데이를 진행한 6~19일 빅데이터를 분석한 결과 1020세대의 주요 품목 구매신장률이 36%를 기록했다고 28일 밝혔다. 같은 기간 3040세대(11%)와 5060세대(6%)의 증가폭을 크게 웃돈다. 이번 조사는 구매가 많이 이뤄지는 대표 카테고리 20개를 기준으로 분석했다....\n",
      "Href :  https://www.news1.kr/articles/5244237\n",
      "\n",
      "Title :  스마일게이트, 中 청두서 'CFS 2023 그랜드 파이널' 개최\n",
      "Press :  월요신문\n",
      "Date :  1일 전\n",
      "Summary :  사진=스마일게이트 ［월요신문=곽민구 기자］스마일게이트 엔터테인먼트는 중국 청두에서 '크로스파이어' 이(E)스포츠 대회인 'CFS 2023 그랜드 파이널' 개막식과 개막전을 개최한다고 29일 밝혔다. CFS 2023 그랜드 파이널은 올해 10주년을 맞아 역대 최대 규모로 개최된다. 4년 만에 크로스파이어...\n",
      "Href :  http://www.wolyo.co.kr/news/articleView.html?idxno=232843\n",
      "\n",
      "Title :  스마일게이트, CFS 2023 그랜드 파이널 中 청두서 개막\n",
      "Press :  한국정경신문\n",
      "Date :  1일 전\n",
      "Summary :  한국정경신문 김명신 기­자 misooya@naver.com 스마일게이트 엔터테인먼트는 중국 청두에서 전세계 최고 권위의 ‘크로스파이어’ e스포츠 대회인 ‘CFS 2023 그랜드 파이널’의 개막식과 개막전을 개최하고 12일 간의 열전에 돌입한다고 29일 밝혔다.CFS 2023 그랜드 파이널’은 올해 10주년을 맞아...\n",
      "Href :  http://kpenews.com/View.aspx?No=3028248\n",
      "\n",
      "Title :  스마일게이트, CFS 2023 그랜드 파이널 개막\n",
      "Press :  게임톡\n",
      "Date :  1일 전\n",
      "Summary :  스마일게이트 엔터테인먼트(대표 장인아)는 중국 청두에서 세계 최고 권위의 '크로스파이어' e스포츠 대회인 'CFS 2023 그랜드 파이널'을 29일 개막했다. CFS 2023 그랜드 파이널'은 올해 10주년을 맞아 역대 최대 규모로 개최된다. 4년 만에 크로스파이어 이스포츠의 인기가 가장 높은 중국 청두에서...\n",
      "Href :  https://www.gametoc.co.kr/news/articleView.html?idxno=77448\n",
      "\n",
      "Title :  투비소프트 자회사 스마일빌드, 창업기획자 등록 완료… \"김포에 벤처생태계 ...\n",
      "Press :  에이빙뉴스\n",
      "Date :  19시간 전\n",
      "Summary :  투비소프트(대표 이경찬)의 자회사 스마일빌드(대표 이호상)가 중소벤처기업부에 창업기획자(액셀러레이터) 등록을 완료했다. 창업기획자란 초기... 스마일빌드는 김포 1호 창업기획자로 등록을 마친 만큼 모회사 투비소프트가 보유한 '테라비즈타워'를 거점으로 엔젤투자, 사업공간 제공, 멘토링 제공 등...\n",
      "Href :  https://kr.aving.net/news/articleView.html?idxno=1786504\n",
      "\n",
      "Title :  스마일게이트, '컨트롤나인'에 투자… '프로젝트 TT' 퍼블리싱 계약 체결\n",
      "Press :  팝콘뉴스\n",
      "Date :  20시간 전\n",
      "Summary :  스마일게이트가 신생 개발사 컨트롤나인에 전략적 투자를 진행하고 컨트롤나인이 개발하는 수집형 턴제 역할수행게임(RPG) '프로젝트 TT(가칭)' 퍼블리싱... 스마일게이트는 이번 투자로 높은 수준의 일본 애니메이션 스타일 지식재산권(IP)을 안정적으로 확보하고 개발사와 긴밀한 협력을 통해 북미, 일본을...\n",
      "Href :  http://www.popcornnews.net/news/articleView.html?idxno=48470\n",
      "\n",
      "Title :  스마일게이트, ‘컨트롤나인’에 투자… 신작 퍼블리싱 계약 체결\n",
      "Press :  바이라인네트워크\n",
      "Date :  18시간 전\n",
      "Summary :  스마일게이트가 니케, 리니지M, 세븐나이츠2 개발 주축이 모인 신생 개발사 컨트롤나인(공동대표 조순구, 권세웅)에 전략적 투자를 진행하고... 스마일게이트는 이번 투자로 높은 수준의 일본 애니메이션 스타일 지식재산권(IP)을 안정적으로 확보하고 개발사와 긴밀한 협력을 통해 북미, 일본을 중심으로 한...\n",
      "Href :  https://byline.network/?p=9004111222520943\n",
      "\n",
      "Title :  스마일게이트, 'CFS 2023 그랜드 파이널' 중국 청두서 개막\n",
      "Press :  아시아타임즈\n",
      "Date :  1일 전\n",
      "Summary :  스마일게이트 엔터테인먼트는 29일 중국 청두에서 크로스파이어 e스포츠 대회인 ‘CFS 2023 그랜드 파이널’의 개막식을 열고 12일간의 열전에 돌입한다고 밝혔다. CFS 2023 그랜드 파이널은 올해 10주년을 맞아 역대 최대 규모로 개최된다. 4년 만에 크로스파이어 e스포츠의 인기가 가장 높은...\n",
      "Href :  https://www.asiatime.co.kr/article/20231129500305\n",
      "\n",
      "Title :  스마일게이트 '크로스파이어' 최강자전 개막...상금 18억원 상회\n",
      "Press :  파이낸셜투데이\n",
      "Date :  1일 전\n",
      "Summary :  스마일게이트 엔터테인먼트는 중국 청두에서 전 세계 최고 권위 '크로스파이어' e스포츠 대회인 'CFS 2023 그랜드 파이널'의 개막식을 29일 개최하고 12일간의 열전에 돌입한다고 이날 밝혔다. 'CFS 2023 그랜드 파이널'은 10주년을 맞은 올해 역대 최대 규모로 개최된다. 크로스파이어...\n",
      "Href :  http://www.ftoday.co.kr/news/articleView.html?idxno=312463\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Press</th>\n",
       "      <th>Date</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Href</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>스마일게이트 '크로스파이어' 최강자전 개막...상금 18억원 상회</td>\n",
       "      <td>파이낸셜투데이</td>\n",
       "      <td>1일 전</td>\n",
       "      <td>스마일게이트 엔터테인먼트는 중국 청두에서 전 세계 최고 권위 '크로스파이어' e스포...</td>\n",
       "      <td>http://www.ftoday.co.kr/news/articleView.html?...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title    Press  Date  \\\n",
       "0  스마일게이트 '크로스파이어' 최강자전 개막...상금 18억원 상회  파이낸셜투데이  1일 전   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  스마일게이트 엔터테인먼트는 중국 청두에서 전 세계 최고 권위 '크로스파이어' e스포...   \n",
       "\n",
       "                                                Href  \n",
       "0  http://www.ftoday.co.kr/news/articleView.html?...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 검색어를 입력받은 후 네이버 뉴스를 크롤링하고 아래와 같이 출력하시오\n",
    "# ---------------\n",
    "# title : xxx\n",
    "# press : xxx\n",
    "# date : xxx\n",
    "# summary : xxx\n",
    "\n",
    "# 모듈 불러오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 초기 검색 내용 및 변수세팅\n",
    "msg = \"스마일\"\n",
    "new_list = []\n",
    "site_num = 2\n",
    "\n",
    "# url 불러오기\n",
    "url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=27&mynews=0&office_type=0&office_section_code=0&news_office_checked=&office_category=0&service_area=0&nso=so:r,p:all,a:all&start={}\".format(msg, (site_num*10 + 1))\n",
    "\n",
    "# html 불러오기\n",
    "res = requests.get(url)\n",
    "\n",
    "# 불러온 내용 soup에 넣기\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# 파싱\n",
    "for item in soup.select('ul.list_news > li'):\n",
    "    title = item.select('a.news_tit')[0].text.strip()\n",
    "    href = item.select('a.news_tit')[0]['href']\n",
    "    press = item.select('div.info_group > a')[0].text.strip()\n",
    "    summary = item.select('div.news_dsc > div > a')[0].text.strip()\n",
    "    date = item.select('div.info_group > span')[0].text.strip()\n",
    "    \n",
    "    print(\"Title : \", title)\n",
    "    print(\"Press : \", press)\n",
    "    print(\"Date : \", date)\n",
    "    print(\"Summary : \", summary)\n",
    "    print(\"Href : \", href)\n",
    "    print()\n",
    "    \n",
    "    new_list = [{\"Title\" : title, \"Press\" : press, \"Date\" : date, \"Summary\" : summary, \"Href\" : href}]\n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "new_pd = pd.DataFrame(new_list)\n",
    "new_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### 검색어를 입력받은 후 네이버 뉴스를 크롤링하고 아래와 같이 출력하시오\n",
    "# ---------------\n",
    "# title : xxx\n",
    "# press : xxx\n",
    "# date : xxx\n",
    "# summary : xxx\n",
    "# content url : xxx\n",
    "# content : xxx\n",
    "\n",
    "# 모듈 불러오기\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from newspaper import Article\n",
    "\n",
    "# 초기 검색 내용 및 변수세팅\n",
    "msg = \"스마일\"\n",
    "new_list = []\n",
    "content_list = []\n",
    "site_num = 1\n",
    "\n",
    "# 네이버 url 불러오기\n",
    "url = \"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={}&sort=0&photo=0&field=0&pd=0&ds=&de=&cluster_rank=27&mynews=0&office_type=0&office_section_code=0&news_office_checked=&office_category=0&service_area=0&nso=so:r,p:all,a:all&start={}\".format(msg, (site_num*10 + 1))\n",
    "\n",
    "# html 불러오기\n",
    "res = requests.get(url)\n",
    "\n",
    "# 불러온 내용 soup에 넣기\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "# 파싱\n",
    "for item in soup.select('ul.list_news > li'):\n",
    "    \n",
    "    print(\"Title : \", title)\n",
    "    print(\"Press : \", press)\n",
    "    print(\"Date : \", date)\n",
    "    print(\"Summary : \", summary)\n",
    "    print(\"URL : \", href)\n",
    "    \n",
    "    res = requests.get(href)\n",
    "    content_soup = BeautifulSoup(res.text,'html.parser')\n",
    "    try:\n",
    "        title = item.select('a.news_tit')[0].text.strip()\n",
    "        href = item.select('a.news_tit')[0]['href']\n",
    "        press = item.select('div.info_group > a')[0].text.strip()\n",
    "        summary = item.select('div.news_dsc > div > a')[0].text.strip()\n",
    "        date = item.select('div.info_group > span')[0].text.strip()\n",
    "    \n",
    "        a = Article(href,language='ko')\n",
    "        a.download()\n",
    "        a.parse()\n",
    "        \n",
    "        content_title = a.title\n",
    "        content_summary = a.text\n",
    "        \n",
    "        new_list.append({\"Title\" : title, \"Press\" : press, \"Date\" : date, \"Summary\" : summary, \"URL\" : href})\n",
    "        content_list.append({\"content_title\" : content_title , \"content_summary\" : content_summary })\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    print()\n",
    "    \n",
    "# csv로 파일 추출하기\n",
    "import pandas as pd\n",
    "\n",
    "new_pd = pd.DataFrame(new_list)\n",
    "content_pd = pd.DataFrame(content_list)\n",
    "\n",
    "new_pd.to_csv(\"new_second.csv\")\n",
    "content_pd.to_csv(\"new_second_content.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
